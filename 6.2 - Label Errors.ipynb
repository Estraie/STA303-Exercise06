{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software installation\n",
    "\n",
    "This lab relies on a couple PyPI packages. If you don't have them installed, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost scikit-learn pandas cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je7P55z4RwX_"
   },
   "source": [
    "## Setup and Data Processing\n",
    "\n",
    "Let's take a look at the dataset used in this lab, a tabular dataset of student grades.\n",
    "\n",
    "The data includes three exam scores (numerical features), a written note (categorical feature with missing values), and a (noisy) letter grade (categorical label). Our aim is to train a model to classify the grade for each student based on the other features.\n",
    "\n",
    "In this dataset, 20% of the grade labels are actually incorrect (the `noisy_letter_grade` column). Synthetic noise was added to this dataset for the purpose of this lab. In this lab, we have access to the true letter grade each student should have received (the `letter_grade` column), which we use for evaluating both the underlying accuracy of model predictions and how well our approach detects which data are mislabeled. We are careful to only use these true grades for evaluation, not for model training.\n",
    "\n",
    "In the real world, you don't have access to the true labels (you only observe the `noisy_letter_grade`, not the true `letter_grade`). So when evaluating models in the real world, you have to be careful to make sure that your test set is free of error (using methods like those covered in this lab, ideally combined with human review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nQVmMBQOS43j",
    "outputId": "54a45659-ecb6-47fe-acfa-d0424027af47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f48f73</td>\n",
       "      <td>53</td>\n",
       "      <td>77</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bd4e7</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e1795d</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb9d7a</td>\n",
       "      <td>61</td>\n",
       "      <td>94</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9acca4</td>\n",
       "      <td>48</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3                    notes letter_grade  \\\n",
       "0  f48f73      53      77      93                      NaN            C   \n",
       "1  0bd4e7      81      64      80  great participation +10            B   \n",
       "2  e1795d      74      88      97                      NaN            B   \n",
       "3  cb9d7a      61      94      78                      NaN            C   \n",
       "4  9acca4      48      90      91                      NaN            C   \n",
       "\n",
       "  noisy_letter_grade  \n",
       "0                  C  \n",
       "1                  B  \n",
       "2                  B  \n",
       "3                  C  \n",
       "4                  C  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/student-grades.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37fd76</td>\n",
       "      <td>99</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>018bff</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b3c9a0</td>\n",
       "      <td>91</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>076d92</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68827d</td>\n",
       "      <td>91</td>\n",
       "      <td>98</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3 notes  letter_grade  noisy_letter_grade\n",
       "0  37fd76      99      59      70     3             3                   3\n",
       "1  018bff      94      41      91     2             1                   1\n",
       "2  b3c9a0      91      74      88     5             1                   1\n",
       "3  076d92       0      79      65     0             4                   4\n",
       "4  68827d      91      98      75     3             2                   2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = df.copy()\n",
    "# Transform letter grades and notes to categorical numbers.\n",
    "# Necessary for XGBoost.\n",
    "df['letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['letter_grade'])\n",
    "df['noisy_letter_grade'] = preprocessing.LabelEncoder().fit_transform(df['noisy_letter_grade'])\n",
    "df['notes'] = preprocessing.LabelEncoder().fit_transform(df[\"notes\"])\n",
    "df['notes'] = df['notes'].astype('category')\n",
    "\n",
    "# Split data for evaluation and set test data.\n",
    "df_train, df_test = train_test_split(df, random_state=0)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVH_iciASD9F"
   },
   "source": [
    "# Get What We Need\n",
    "\n",
    "To apply confident learning (the technique explained in today's lecture), we need to obtain [**out-of-sample** predicted probabilities](https://docs.cleanlab.ai/stable/tutorials/pred_probs_cross_val.html#out-of-sample-predicted-probabilities) for all of our data. To do this, we can use K-fold cross validation: for each fold, we will train on some subset of our data and get predictions on the rest of the data that was _not_ used for training.\n",
    "\n",
    "We need to choose a model in order to do this. For this lab, we'll use [XGBoost](https://xgboost.readthedocs.io/), a library implementing gradient-boosted decision trees, a class of model commonly used for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCS19IqJsQUL",
    "outputId": "fd89b945-6793-4b3a-a017-7b19f8e6a29b"
   },
   "outputs": [],
   "source": [
    "#Prepare test data (this will not change across models)\n",
    "test_data = df_test.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "test_labels = df_test['letter_grade']\n",
    "\n",
    "# Prepare training data (remove labels from the dataframe) and labels\n",
    "train_data = df_train.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "train_labels = df_train['noisy_letter_grade']\n",
    "\n",
    "# XGBoost(experimental) supports categorical data.\n",
    "# Here we use default hyperparameters for simplicity.\n",
    "# Get out-of-sample predicted probabilities and check model accuracy.\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish Baseline Accuracy\n",
    "\n",
    "Let's also train our model on the noisy data and evaluate it on our seperate test data to establish a baseline to compare our final results with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 79.2%\n"
     ]
    }
   ],
   "source": [
    "# Train model on original, possibly noisy data.\n",
    "model.fit(train_data, train_labels)\n",
    "\n",
    "# Evaluate model on test split with ground truth labels.\n",
    "preds = model.predict(test_data)\n",
    "acc_original = accuracy_score(preds, test_labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: getting out-of-sample predicted probabilities\n",
    "\n",
    "Compute out-of-sample predicted probabilities for every data point. You can do this manually using for loops and multiple invocations of model training and prediction, or you can use scikit-learn's [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) (if you're using this function, take a look at the documentations, and in particular, the `method=` keyword argument). Let's use 10-folds (`cv=10`) for a balance of accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00445347, 0.0336567 , 0.1216571 , 0.75258416, 0.08764851],\n",
       "        [0.8702499 , 0.07702546, 0.03176929, 0.01376026, 0.00719508],\n",
       "        [0.00350568, 0.88157785, 0.00146268, 0.08411037, 0.02934347],\n",
       "        ...,\n",
       "        [0.02096226, 0.02510118, 0.88480276, 0.02610611, 0.04302767],\n",
       "        [0.01524511, 0.05465487, 0.4861005 , 0.00636221, 0.43763727],\n",
       "        [0.00533796, 0.3721377 , 0.00089144, 0.06241276, 0.5592202 ]],\n",
       "       dtype=float32),\n",
       " (705, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_probs should be a Nx5 matrix of out-of-sample predicted probabilities, with N = len(data)\n",
    "\n",
    "pred_probs = cross_val_predict(model, train_data, train_labels, method='predict_proba', cv=10)\n",
    "pred_probs, pred_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding label issues automatically\n",
    "\n",
    "We count label issues using confident learning. First, we need to compute class thresholds for the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: computing class thresholds\n",
    "\n",
    "Implement the Confident Learning algorithm for computing class thresholds for the 5 classes. You can refer to slide 26 from today's lecture or see equation 2 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The class threshold for each class is the model's expected (average) self-confidence for each class. In other words, to compute the threshold for a particular class, you can average the predicted probability for that class, for all datapoints that are labeled with that particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00445347, 0.0336567 , 0.1216571 , 0.75258416, 0.08764851],\n",
       "        [0.8702499 , 0.07702546, 0.03176929, 0.01376026, 0.00719508],\n",
       "        [0.00350568, 0.88157785, 0.00146268, 0.08411037, 0.02934347],\n",
       "        ...,\n",
       "        [0.02096226, 0.02510118, 0.88480276, 0.02610611, 0.04302767],\n",
       "        [0.01524511, 0.05465487, 0.4861005 , 0.00636221, 0.43763727],\n",
       "        [0.00533796, 0.3721377 , 0.00089144, 0.06241276, 0.5592202 ]],\n",
       "       dtype=float32),\n",
       " 0      3\n",
       " 1      1\n",
       " 2      1\n",
       " 3      4\n",
       " 4      2\n",
       "       ..\n",
       " 700    4\n",
       " 701    1\n",
       " 702    0\n",
       " 703    1\n",
       " 704    4\n",
       " Name: noisy_letter_grade, Length: 705, dtype: int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_labels.to_numpy() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_thresholds(pred_probs: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    return np.array([1 / sum(labels == c) * sum(pred_probs[(labels == c), c]) for c in range(5)], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.70661993, 0.63892676, 0.63531261, 0.45481032, 0.44689162]), True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be a numpy array of length 5\n",
    "thresholds = compute_class_thresholds(pred_probs, train_labels.to_numpy())\n",
    "thresholds, isinstance(thresholds, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: constructing the confident joint\n",
    "\n",
    "Next, we compute the confident joint, a matrix that counts the number of label errors for each noisy label $\\tilde{y}$ and true label $y^*$. You can follow the algorithm that we walked through in slide 27 from today's lecture, or see equation 1 in [this paper](https://jair.org/index.php/jair/article/view/12125).\n",
    "\n",
    "The confident joint C is a K x K matrix (with K = 5 for this dataset), where `C[i][j]` is an estimate of the count of the number of data points with noisy label `i` and true label `j`. From lecture, recall that we put a data point in bin `(i, j)` if its given label is `i`, and its predicted probability for class `j` is above the threshold for class `j` (`thresholds[j]`). Each data point should only go in a single bin; if a data point's predicted probability is above the class threshold for multiple classes, it goes in the bin for which it has the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confident_joint(pred_probs: np.ndarray, labels: np.ndarray, thresholds: np.ndarray) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return np.array([\n",
    "        [\n",
    "            sum((pred_probs[:, j] > thresholds[j]) & (labels == i))\n",
    "            for j in range(5)\n",
    "        ]\n",
    "        for i in range(5)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[116,   3,  12,   5,  14],\n",
       "       [  7, 109,   3,  24,   7],\n",
       "       [  0,   3,  77,   6,   8],\n",
       "       [  2,  25,   5,  56,   4],\n",
       "       [ 16,  14,  12,   7,  51]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = compute_confident_joint(pred_probs, train_labels.to_numpy(), thresholds)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: count the number of label issues\n",
    "\n",
    "Now that we have the confident joint C, we can count the estimated number of label issues in our dataset. Recall that this is the sum of the off-diagonal entries (the cases where we estimate that a label has been flipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_label_issues = sum([sum([C[i][j] for j in range(5) if i != j]) for i in range(5)])\n",
    "num_label_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated noise rate: 25.1%\n"
     ]
    }
   ],
   "source": [
    "print('Estimated noise rate: {:.1f}%'.format(100*num_label_issues / pred_probs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: filter out label issues\n",
    "\n",
    "In this lab, our approach to identifying issues is to rank the data points by a score (\"self-confidence\", the model's predicted probability for a data point's given label) and then take the top `num_label_issues` of those.\n",
    "\n",
    "First, we want to compute the model's _self-confidence_ for each data point. For a data point `i`, that is `pred_probs[i, labels[i]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75258416,\n",
       " 0.07702546,\n",
       " 0.88157785,\n",
       " 0.2157897,\n",
       " 0.15904717,\n",
       " 0.034208495,\n",
       " 0.74240506,\n",
       " 0.98297095,\n",
       " 0.9822227,\n",
       " 0.06445531,\n",
       " 0.61537373,\n",
       " 0.020835672,\n",
       " 0.9890243,\n",
       " 0.9854848,\n",
       " 0.97718734,\n",
       " 0.8674533,\n",
       " 0.002863215,\n",
       " 0.58845896,\n",
       " 0.99375397,\n",
       " 0.72261715,\n",
       " 0.9926609,\n",
       " 0.67272645,\n",
       " 0.8653403,\n",
       " 0.7672698,\n",
       " 0.98744756,\n",
       " 0.86155856,\n",
       " 0.15069957,\n",
       " 0.37150386,\n",
       " 0.68537515,\n",
       " 0.9935415,\n",
       " 0.022344857,\n",
       " 0.6455186,\n",
       " 0.67208755,\n",
       " 0.6140305,\n",
       " 0.05843262,\n",
       " 0.93352,\n",
       " 0.93162894,\n",
       " 0.8131347,\n",
       " 0.9755539,\n",
       " 0.0442314,\n",
       " 0.8802679,\n",
       " 0.058346037,\n",
       " 0.016612608,\n",
       " 0.18220071,\n",
       " 0.989099,\n",
       " 0.6160011,\n",
       " 0.038626477,\n",
       " 0.5968413,\n",
       " 0.9951367,\n",
       " 0.8004122,\n",
       " 0.4080649,\n",
       " 0.65225065,\n",
       " 0.02932973,\n",
       " 0.9871999,\n",
       " 0.021419862,\n",
       " 0.9946413,\n",
       " 0.99122053,\n",
       " 0.643335,\n",
       " 0.13058646,\n",
       " 0.95004976,\n",
       " 0.14528996,\n",
       " 0.06288553,\n",
       " 0.9863202,\n",
       " 0.99056494,\n",
       " 0.9200623,\n",
       " 0.3081974,\n",
       " 0.27729222,\n",
       " 0.9529175,\n",
       " 0.79340434,\n",
       " 0.9871585,\n",
       " 0.9775735,\n",
       " 0.9441286,\n",
       " 0.7858582,\n",
       " 0.90613616,\n",
       " 0.03042209,\n",
       " 0.0031981363,\n",
       " 0.23279856,\n",
       " 0.9766905,\n",
       " 0.98974085,\n",
       " 0.9661483,\n",
       " 0.013509531,\n",
       " 0.081957504,\n",
       " 0.043473326,\n",
       " 0.5446456,\n",
       " 0.010283756,\n",
       " 0.99412054,\n",
       " 0.9324432,\n",
       " 0.9598993,\n",
       " 0.8140568,\n",
       " 0.95588416,\n",
       " 0.96609724,\n",
       " 0.99688333,\n",
       " 0.01083539,\n",
       " 0.52340287,\n",
       " 0.99272984,\n",
       " 0.005273223,\n",
       " 0.8510636,\n",
       " 0.8997682,\n",
       " 0.06791602,\n",
       " 0.941519,\n",
       " 0.07746782,\n",
       " 0.23530206,\n",
       " 0.9687264,\n",
       " 0.941718,\n",
       " 0.25938025,\n",
       " 0.9721041,\n",
       " 0.42457795,\n",
       " 0.98278517,\n",
       " 0.96252394,\n",
       " 0.92410505,\n",
       " 0.9169502,\n",
       " 0.0031283516,\n",
       " 0.11737498,\n",
       " 0.9836336,\n",
       " 0.8093769,\n",
       " 0.9529284,\n",
       " 0.6891993,\n",
       " 0.910092,\n",
       " 0.67537177,\n",
       " 0.2956304,\n",
       " 0.014856907,\n",
       " 0.85254735,\n",
       " 0.9622873,\n",
       " 0.017443199,\n",
       " 0.18902853,\n",
       " 0.3011698,\n",
       " 0.94723606,\n",
       " 0.98165864,\n",
       " 0.95580924,\n",
       " 0.039689712,\n",
       " 0.2616718,\n",
       " 0.99848264,\n",
       " 0.9177332,\n",
       " 0.85100967,\n",
       " 0.8594836,\n",
       " 0.77283525,\n",
       " 0.9973189,\n",
       " 0.98811764,\n",
       " 0.9881506,\n",
       " 0.90028334,\n",
       " 0.020735767,\n",
       " 0.99181765,\n",
       " 0.13835894,\n",
       " 0.99638075,\n",
       " 0.8792931,\n",
       " 0.06344278,\n",
       " 0.91682655,\n",
       " 0.052858472,\n",
       " 0.030181054,\n",
       " 0.62129,\n",
       " 0.79618925,\n",
       " 0.96793383,\n",
       " 0.045804128,\n",
       " 0.9988292,\n",
       " 0.8989268,\n",
       " 0.43150482,\n",
       " 0.89697415,\n",
       " 0.9961539,\n",
       " 0.9275337,\n",
       " 0.6468659,\n",
       " 0.5079363,\n",
       " 0.99491113,\n",
       " 0.8810715,\n",
       " 0.137463,\n",
       " 0.8695062,\n",
       " 0.59024906,\n",
       " 0.8424355,\n",
       " 0.028155262,\n",
       " 0.72129667,\n",
       " 0.04218273,\n",
       " 0.933529,\n",
       " 0.90141755,\n",
       " 0.4804155,\n",
       " 0.1693991,\n",
       " 0.72070163,\n",
       " 0.036926113,\n",
       " 0.95778024,\n",
       " 0.05080184,\n",
       " 0.86446816,\n",
       " 0.25414124,\n",
       " 0.049287822,\n",
       " 0.88006943,\n",
       " 0.004292074,\n",
       " 0.56402236,\n",
       " 0.82326186,\n",
       " 0.9009251,\n",
       " 0.0015638226,\n",
       " 0.9613926,\n",
       " 0.9052932,\n",
       " 0.99781585,\n",
       " 0.00046710717,\n",
       " 0.3962035,\n",
       " 0.7472642,\n",
       " 0.00818884,\n",
       " 0.90397143,\n",
       " 0.57480335,\n",
       " 0.00016090162,\n",
       " 0.15822986,\n",
       " 0.97668284,\n",
       " 0.0013233853,\n",
       " 0.076955356,\n",
       " 0.6961146,\n",
       " 0.99761486,\n",
       " 0.945504,\n",
       " 0.008365943,\n",
       " 0.9705157,\n",
       " 0.00020992746,\n",
       " 0.77005893,\n",
       " 0.7817256,\n",
       " 0.21798642,\n",
       " 0.9545164,\n",
       " 0.9596447,\n",
       " 0.91497254,\n",
       " 0.9651065,\n",
       " 0.023334641,\n",
       " 0.00019747032,\n",
       " 0.026442342,\n",
       " 0.88666683,\n",
       " 0.96155447,\n",
       " 0.59137917,\n",
       " 0.96471393,\n",
       " 0.06915255,\n",
       " 0.6848908,\n",
       " 0.058167815,\n",
       " 0.9844899,\n",
       " 0.9862444,\n",
       " 0.04168052,\n",
       " 0.90677667,\n",
       " 0.9224145,\n",
       " 0.030771866,\n",
       " 0.987738,\n",
       " 0.75273174,\n",
       " 0.2669185,\n",
       " 0.7328653,\n",
       " 0.0049800016,\n",
       " 0.12614897,\n",
       " 0.90677977,\n",
       " 0.22108921,\n",
       " 0.99745196,\n",
       " 0.027301693,\n",
       " 0.96271694,\n",
       " 0.3664914,\n",
       " 0.9155993,\n",
       " 0.0078545,\n",
       " 0.9910363,\n",
       " 0.9429511,\n",
       " 0.752426,\n",
       " 0.9124727,\n",
       " 0.9032443,\n",
       " 0.046919752,\n",
       " 0.37861684,\n",
       " 0.8070529,\n",
       " 0.013044612,\n",
       " 0.06747002,\n",
       " 0.450826,\n",
       " 0.031577677,\n",
       " 0.6258793,\n",
       " 0.9760307,\n",
       " 0.5859559,\n",
       " 0.7102988,\n",
       " 0.97937316,\n",
       " 0.9586491,\n",
       " 0.010555525,\n",
       " 0.98235184,\n",
       " 0.9983942,\n",
       " 0.015945818,\n",
       " 0.0049406514,\n",
       " 0.8739578,\n",
       " 0.79770166,\n",
       " 0.9323615,\n",
       " 0.27431685,\n",
       " 0.5829486,\n",
       " 0.00050859497,\n",
       " 0.91413474,\n",
       " 0.98493284,\n",
       " 0.7752942,\n",
       " 0.81404495,\n",
       " 0.6529934,\n",
       " 0.43027714,\n",
       " 0.03760359,\n",
       " 0.55144286,\n",
       " 0.9307487,\n",
       " 0.9717856,\n",
       " 0.053388897,\n",
       " 0.9899755,\n",
       " 0.9427467,\n",
       " 0.29512978,\n",
       " 0.9995524,\n",
       " 0.45910347,\n",
       " 0.16178252,\n",
       " 0.99257326,\n",
       " 0.08241404,\n",
       " 0.5354317,\n",
       " 0.96637404,\n",
       " 0.9746123,\n",
       " 0.82236886,\n",
       " 0.07875186,\n",
       " 0.964527,\n",
       " 0.95724326,\n",
       " 0.9691059,\n",
       " 0.67130876,\n",
       " 0.94071627,\n",
       " 0.97918725,\n",
       " 0.99720675,\n",
       " 0.013250394,\n",
       " 0.7792001,\n",
       " 0.98388356,\n",
       " 0.60860693,\n",
       " 0.9917529,\n",
       " 0.0026812153,\n",
       " 0.3492363,\n",
       " 0.89271194,\n",
       " 0.94587755,\n",
       " 0.5722007,\n",
       " 0.8114026,\n",
       " 0.8251798,\n",
       " 0.95842737,\n",
       " 0.9844428,\n",
       " 0.5727535,\n",
       " 0.452348,\n",
       " 0.69055986,\n",
       " 0.51980627,\n",
       " 0.0032806615,\n",
       " 0.19001584,\n",
       " 0.7023639,\n",
       " 0.019389186,\n",
       " 0.9884465,\n",
       " 0.987632,\n",
       " 0.018455142,\n",
       " 0.6403464,\n",
       " 0.28180003,\n",
       " 0.96943116,\n",
       " 0.9525841,\n",
       " 0.9818009,\n",
       " 0.94464135,\n",
       " 0.98651403,\n",
       " 0.97607225,\n",
       " 0.845934,\n",
       " 0.9322303,\n",
       " 0.8597785,\n",
       " 0.7510898,\n",
       " 0.996711,\n",
       " 0.6849408,\n",
       " 0.14225543,\n",
       " 0.22165139,\n",
       " 0.006805496,\n",
       " 0.24899518,\n",
       " 0.2474988,\n",
       " 0.019651067,\n",
       " 0.012294093,\n",
       " 0.025858875,\n",
       " 0.9745321,\n",
       " 0.8598805,\n",
       " 0.87813044,\n",
       " 0.9947577,\n",
       " 0.09368152,\n",
       " 0.0006628942,\n",
       " 0.95498115,\n",
       " 0.9674128,\n",
       " 0.0019707994,\n",
       " 0.8910389,\n",
       " 0.08963965,\n",
       " 0.91540587,\n",
       " 0.77801496,\n",
       " 0.987807,\n",
       " 0.004544454,\n",
       " 0.974898,\n",
       " 0.976824,\n",
       " 0.0031105385,\n",
       " 0.96888405,\n",
       " 0.94307256,\n",
       " 0.9632387,\n",
       " 0.007796653,\n",
       " 0.97888196,\n",
       " 0.9890013,\n",
       " 0.9517586,\n",
       " 0.24391083,\n",
       " 0.230371,\n",
       " 0.021774713,\n",
       " 0.55272484,\n",
       " 0.117025666,\n",
       " 0.0029994312,\n",
       " 0.1790143,\n",
       " 0.9856924,\n",
       " 0.08779412,\n",
       " 0.07554037,\n",
       " 0.93705624,\n",
       " 0.37799788,\n",
       " 0.087326005,\n",
       " 0.80719393,\n",
       " 0.99789894,\n",
       " 0.85419965,\n",
       " 0.63926727,\n",
       " 0.23775466,\n",
       " 0.9890261,\n",
       " 0.99130553,\n",
       " 0.02880506,\n",
       " 0.99975413,\n",
       " 0.0048361337,\n",
       " 0.92644805,\n",
       " 0.7995181,\n",
       " 0.0685008,\n",
       " 0.97720885,\n",
       " 0.01060265,\n",
       " 0.0036483016,\n",
       " 0.010146646,\n",
       " 0.99715865,\n",
       " 0.8560521,\n",
       " 0.7795215,\n",
       " 0.00012097244,\n",
       " 0.59245676,\n",
       " 0.9965815,\n",
       " 0.2957083,\n",
       " 0.025956115,\n",
       " 0.5908351,\n",
       " 0.9971789,\n",
       " 0.023248313,\n",
       " 0.42515084,\n",
       " 0.99513936,\n",
       " 0.18878458,\n",
       " 0.5234453,\n",
       " 0.44142628,\n",
       " 0.9635067,\n",
       " 0.26819605,\n",
       " 0.24439476,\n",
       " 0.55889183,\n",
       " 0.5291046,\n",
       " 0.9848667,\n",
       " 0.5106833,\n",
       " 0.9897453,\n",
       " 0.97454643,\n",
       " 0.9971028,\n",
       " 0.065111,\n",
       " 0.9945446,\n",
       " 0.7568955,\n",
       " 0.639386,\n",
       " 0.122739926,\n",
       " 0.91849303,\n",
       " 0.98876625,\n",
       " 0.9957242,\n",
       " 0.55526596,\n",
       " 0.48129076,\n",
       " 0.91198313,\n",
       " 0.605601,\n",
       " 0.9973001,\n",
       " 0.2698232,\n",
       " 0.011995352,\n",
       " 0.27669704,\n",
       " 0.13499676,\n",
       " 0.9974528,\n",
       " 0.11113175,\n",
       " 0.77801764,\n",
       " 0.88344973,\n",
       " 0.968938,\n",
       " 0.30599913,\n",
       " 0.93552184,\n",
       " 0.0644365,\n",
       " 0.9947377,\n",
       " 0.8471483,\n",
       " 0.14261372,\n",
       " 0.9927499,\n",
       " 0.97794914,\n",
       " 0.012056539,\n",
       " 0.8350844,\n",
       " 0.9567094,\n",
       " 0.9850268,\n",
       " 0.60197586,\n",
       " 0.74114954,\n",
       " 0.008787699,\n",
       " 0.00074768794,\n",
       " 0.2726764,\n",
       " 0.010838905,\n",
       " 0.97474426,\n",
       " 0.003332417,\n",
       " 0.028151572,\n",
       " 0.85468054,\n",
       " 0.037929833,\n",
       " 0.009047921,\n",
       " 0.9982822,\n",
       " 0.97796774,\n",
       " 0.40683955,\n",
       " 0.89570045,\n",
       " 0.9227074,\n",
       " 0.96777356,\n",
       " 0.90994555,\n",
       " 0.9901759,\n",
       " 0.31110552,\n",
       " 0.43933463,\n",
       " 0.7321591,\n",
       " 0.8665481,\n",
       " 0.9218705,\n",
       " 0.10375744,\n",
       " 0.70955116,\n",
       " 0.12424158,\n",
       " 0.043149117,\n",
       " 0.6413317,\n",
       " 0.0070520034,\n",
       " 0.0016269987,\n",
       " 0.14382067,\n",
       " 0.5734101,\n",
       " 0.9947365,\n",
       " 0.9927267,\n",
       " 0.11763443,\n",
       " 0.97782147,\n",
       " 0.7504535,\n",
       " 0.811438,\n",
       " 0.3144667,\n",
       " 0.7946415,\n",
       " 0.95326144,\n",
       " 0.92121303,\n",
       " 0.004225892,\n",
       " 0.006987802,\n",
       " 0.99761415,\n",
       " 0.06377651,\n",
       " 0.84381914,\n",
       " 0.86931705,\n",
       " 0.9814643,\n",
       " 0.5149334,\n",
       " 0.92766136,\n",
       " 0.13034804,\n",
       " 0.8558653,\n",
       " 0.30405432,\n",
       " 0.97466725,\n",
       " 0.9861644,\n",
       " 0.01318405,\n",
       " 0.70123106,\n",
       " 0.4798399,\n",
       " 0.12823926,\n",
       " 0.95656925,\n",
       " 0.9773004,\n",
       " 0.99383634,\n",
       " 0.013930963,\n",
       " 0.17174982,\n",
       " 0.9562388,\n",
       " 0.3723485,\n",
       " 0.9196128,\n",
       " 0.957529,\n",
       " 0.93549186,\n",
       " 0.9477548,\n",
       " 0.99094534,\n",
       " 0.9989674,\n",
       " 0.00011296711,\n",
       " 0.9803955,\n",
       " 0.98215693,\n",
       " 0.01808646,\n",
       " 0.1010839,\n",
       " 0.9941228,\n",
       " 0.054025464,\n",
       " 0.9858247,\n",
       " 0.055868376,\n",
       " 0.851116,\n",
       " 0.9887107,\n",
       " 0.42480472,\n",
       " 0.08309053,\n",
       " 0.17905194,\n",
       " 0.9816553,\n",
       " 0.98152333,\n",
       " 0.974922,\n",
       " 0.0014709227,\n",
       " 0.9973152,\n",
       " 0.7934749,\n",
       " 0.32747138,\n",
       " 0.0029378994,\n",
       " 0.99847454,\n",
       " 0.107728615,\n",
       " 0.81774616,\n",
       " 0.6193533,\n",
       " 0.8915965,\n",
       " 0.008713165,\n",
       " 0.99099934,\n",
       " 0.9935428,\n",
       " 0.99345136,\n",
       " 0.99756,\n",
       " 0.80548364,\n",
       " 0.9017658,\n",
       " 0.9866409,\n",
       " 0.99827814,\n",
       " 0.9222048,\n",
       " 0.002006441,\n",
       " 0.93157506,\n",
       " 0.0023001432,\n",
       " 0.9878001,\n",
       " 0.72407645,\n",
       " 0.6407823,\n",
       " 0.82013273,\n",
       " 0.03306592,\n",
       " 0.99379784,\n",
       " 0.669007,\n",
       " 0.8856561,\n",
       " 0.96749204,\n",
       " 0.97955865,\n",
       " 0.0030116702,\n",
       " 0.8215043,\n",
       " 0.9795307,\n",
       " 0.80245066,\n",
       " 0.08246165,\n",
       " 0.0644678,\n",
       " 0.6983454,\n",
       " 0.9947131,\n",
       " 0.025619952,\n",
       " 0.007841111,\n",
       " 0.3893355,\n",
       " 0.9768311,\n",
       " 0.01354022,\n",
       " 0.48896658,\n",
       " 0.6769002,\n",
       " 0.64439404,\n",
       " 0.78578305,\n",
       " 0.0022911578,\n",
       " 0.00081496034,\n",
       " 0.91416514,\n",
       " 0.45735845,\n",
       " 0.00626123,\n",
       " 0.97094643,\n",
       " 0.027245589,\n",
       " 0.017042208,\n",
       " 0.1865518,\n",
       " 0.5979163,\n",
       " 0.99339193,\n",
       " 0.8975805,\n",
       " 0.35141057,\n",
       " 0.5384355,\n",
       " 0.72599375,\n",
       " 0.6059058,\n",
       " 0.16819274,\n",
       " 0.9787539,\n",
       " 0.95257676,\n",
       " 0.00046867985,\n",
       " 0.003961908,\n",
       " 0.9788455,\n",
       " 0.983386,\n",
       " 0.8057433,\n",
       " 0.98025906,\n",
       " 0.053029418,\n",
       " 0.97946733,\n",
       " 0.58730173,\n",
       " 0.37033418,\n",
       " 0.30686805,\n",
       " 0.9720452,\n",
       " 0.9863262,\n",
       " 0.004180543,\n",
       " 0.8566583,\n",
       " 0.995059,\n",
       " 0.36949727,\n",
       " 0.9193981,\n",
       " 0.83473855,\n",
       " 0.99442106,\n",
       " 0.0021773705,\n",
       " 0.023901384,\n",
       " 0.8957644,\n",
       " 0.95560277,\n",
       " 0.95978093,\n",
       " 0.9972377,\n",
       " 0.9624513,\n",
       " 0.43723065,\n",
       " 0.8802367,\n",
       " 0.9882775,\n",
       " 0.5701262,\n",
       " 0.17359695,\n",
       " 0.3391357,\n",
       " 0.97694606,\n",
       " 0.8938743,\n",
       " 0.7501745,\n",
       " 0.75053316,\n",
       " 0.6871781,\n",
       " 0.9902586,\n",
       " 0.95255125,\n",
       " 0.70166814,\n",
       " 0.92456716,\n",
       " 0.007753923,\n",
       " 0.3047791,\n",
       " 0.0017434135,\n",
       " 0.704496,\n",
       " 0.599733,\n",
       " 0.22190213,\n",
       " 0.95941633,\n",
       " 0.54591477,\n",
       " 0.103471324,\n",
       " 0.91057336,\n",
       " 0.08063864,\n",
       " 0.94123024,\n",
       " 0.004833235,\n",
       " 0.9625133,\n",
       " 0.20003976,\n",
       " 0.9874964,\n",
       " 0.010781546,\n",
       " 0.8695642,\n",
       " 0.61632144,\n",
       " 0.32861462,\n",
       " 0.99449325,\n",
       " 0.56338644,\n",
       " 0.99351513,\n",
       " 0.5935202,\n",
       " 0.9931629,\n",
       " 0.65493125,\n",
       " 0.014007325,\n",
       " 0.88129294,\n",
       " 0.9965282,\n",
       " 0.23612693,\n",
       " 0.030095179,\n",
       " 0.34953594,\n",
       " 0.9854858,\n",
       " 0.02096226,\n",
       " 0.054654866,\n",
       " 0.5592202]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be a numpy array of length 941 of probabilities\n",
    "self_confidences = [pred_probs[i, train_labels[i]] for i in range(len(pred_probs))]\n",
    "self_confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we rank the _indices_ of the data points by the self-confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([541, 409, 196, 215, 206, 190, 627, 272, 356, 469, 609, 199, 558,\n",
       "       186, 497, 671, 359, 578, 647, 608, 580, 309,  16, 562, 381, 591,\n",
       "       368, 111,  75, 322, 473, 404, 628, 640, 510, 182, 365, 681, 398,\n",
       "       266, 234,  95, 612, 345, 511, 496, 669, 372, 600, 243, 193, 204,\n",
       "       568, 468, 477, 405,  84, 262, 403, 685,  92, 471, 446, 462, 349,\n",
       "       252, 524, 304,  80, 603, 531, 695, 120, 265,  42, 615, 123, 544,\n",
       "       328, 325, 348, 140,  11, 702,  54, 378,  30, 416, 214, 648, 599,\n",
       "       350, 413, 216, 614, 239, 474, 167, 396,  52, 699, 148,  74, 229,\n",
       "       255, 585,   5, 175, 279, 476,  46, 129, 226, 169, 494,  82,  39,\n",
       "       152, 249, 180, 177, 147, 633, 283, 547, 703, 549, 223,  41,  34,\n",
       "        61, 145, 513, 456,   9, 596, 432, 253,  98, 401, 221, 385, 200,\n",
       "         1, 100, 296, 679,  81, 291, 595, 553, 388, 384, 361, 355, 545,\n",
       "       677, 491, 564, 450, 380, 112, 502, 436, 493, 235, 527, 519,  58,\n",
       "       448, 163, 142, 343, 459, 498,  60,  26, 197,   4, 289, 624, 173,\n",
       "       532, 658, 382, 554,  43, 616, 419, 124, 323, 683,   3, 209, 237,\n",
       "       344, 674, 377,  76, 101, 698, 393, 376, 424, 347, 346, 179, 104,\n",
       "       130, 232, 423, 445, 470, 270, 447,  66, 330, 286, 119, 412, 125,\n",
       "       521, 670, 454, 637,  65, 486, 506, 561, 688, 659, 310, 700, 620,\n",
       "       241, 643, 636,  27, 534, 387, 250, 601, 191, 480,  50, 106, 552,\n",
       "       417, 278, 155, 654, 487, 421, 254, 319, 611, 288, 526, 172, 441,\n",
       "       604, 160, 428, 517, 321,  93, 420, 426, 292, 621,  83, 676, 280,\n",
       "       379, 440, 425, 704, 690, 183, 657, 313, 318, 499, 195, 271, 258,\n",
       "       635,  17, 165, 414, 219, 410, 692,  47, 617, 673, 466, 443, 623,\n",
       "       307,  33,  10,  45, 687, 566, 149, 256, 392, 435, 329, 583, 495,\n",
       "        57, 606,  31, 159,  51, 277, 694, 587, 300,  32,  21, 118, 605,\n",
       "       222, 342,  28, 664, 116, 320, 201, 597, 525, 667, 324, 672, 492,\n",
       "       259, 174, 168,  19, 582, 622, 488, 233, 467,   6, 192, 662, 504,\n",
       "       663, 340, 246,   0, 231, 434,  23, 207, 135, 275, 363, 451, 305,\n",
       "       408, 208, 607,  72,  68, 560, 507, 150, 268, 400,  49, 594, 573,\n",
       "       631, 251, 389, 114, 314, 505,  37, 276,  88, 565, 584, 592, 295,\n",
       "       184, 315, 645, 463, 166, 514, 337, 458, 133,  96, 550, 121, 391,\n",
       "       475, 520, 407, 641, 134, 339, 352,  25, 178,  22, 489,  15, 515,\n",
       "       164, 686, 267, 353, 144, 181, 655,  40, 162, 696,   2, 452, 588,\n",
       "       217, 360, 567, 311, 661, 481, 649, 156, 619, 154,  97, 139, 185,\n",
       "       171, 574, 248, 194, 188,  73, 227, 236, 484, 117, 678, 442, 247,\n",
       "       273, 610, 212, 362, 242, 146, 110, 132, 437, 644, 535,  64, 509,\n",
       "       490, 577, 228, 482, 109, 668, 399, 158, 518, 281, 579,  36, 338,\n",
       "       269,  86,  35, 170, 537, 455, 386, 301, 680,  99, 103, 285, 245,\n",
       "       370,  71, 334, 203, 312, 126, 538,  59, 375, 666, 626, 332,  67,\n",
       "       115, 508, 210, 357, 650, 128,  89, 533, 528, 464, 298, 536, 176,\n",
       "       316, 261, 675, 211, 651,  87, 187, 218, 122, 653, 682, 108, 240,\n",
       "       371, 422, 297, 220, 213,  90,  79, 293, 358, 589, 483, 151, 102,\n",
       "       369, 453, 299, 331, 205, 613, 282, 638, 105, 351, 430, 294, 522,\n",
       "       472, 366, 557,  38, 257, 336, 198,  77, 367, 602, 660,  14, 402,\n",
       "       529,  70, 503, 461, 479, 625, 629, 373, 302, 260, 634, 593, 590,\n",
       "       632, 542, 516, 556, 555, 127, 333, 543,   8, 263, 107,   7, 630,\n",
       "       113, 306, 317, 224, 427, 274, 465,  13, 701, 383, 548, 523, 225,\n",
       "        62, 639, 335, 575,  69,  53,  24, 684, 327, 230, 581, 364, 137,\n",
       "       138, 656, 326, 551, 438, 374,  12, 394,  44,  78, 429, 284, 485,\n",
       "       665,  63, 539, 569, 244,  56, 395, 308, 141, 290,  20, 501,  94,\n",
       "       460, 693, 618, 571, 691,  29, 570,  18, 586, 530,  85, 546, 646,\n",
       "       689, 433,  55, 598, 500, 457, 354, 161, 642,  48, 418, 439, 157,\n",
       "       143, 697, 411, 341,  91, 431, 406, 415, 303, 652, 444, 559, 136,\n",
       "       238, 449, 572, 512, 202, 189, 390, 576, 478, 264, 563, 131, 153,\n",
       "       540, 287, 397], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be a numpy array of length 941 of integer indices\n",
    "ranked_indices = np.argsort(self_confidences)\n",
    "# ranked_indices = np.empty_like(sorted_indices)\n",
    "# ranked_indices[sorted_indices] = np.arange(len(self_confidences))\n",
    "ranked_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the indices of label issues as the top `num_label_issues` items in the `ranked_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([541, 409, 196, 215, 206, 190, 627, 272, 356, 469, 609, 199, 558,\n",
       "       186, 497, 671, 359, 578, 647, 608, 580, 309,  16, 562, 381, 591,\n",
       "       368, 111,  75, 322, 473, 404, 628, 640, 510, 182, 365, 681, 398,\n",
       "       266, 234,  95, 612, 345, 511, 496, 669, 372, 600, 243, 193, 204,\n",
       "       568, 468, 477, 405,  84, 262, 403, 685,  92, 471, 446, 462, 349,\n",
       "       252, 524, 304,  80, 603, 531, 695, 120, 265,  42, 615, 123, 544,\n",
       "       328, 325, 348, 140,  11, 702,  54, 378,  30, 416, 214, 648, 599,\n",
       "       350, 413, 216, 614, 239, 474, 167, 396,  52, 699, 148,  74, 229,\n",
       "       255, 585,   5, 175, 279, 476,  46, 129, 226, 169, 494,  82,  39,\n",
       "       152, 249, 180, 177, 147, 633, 283, 547, 703, 549, 223,  41,  34,\n",
       "        61, 145, 513, 456,   9, 596, 432, 253,  98, 401, 221, 385, 200,\n",
       "         1, 100, 296, 679,  81, 291, 595, 553, 388, 384, 361, 355, 545,\n",
       "       677, 491, 564, 450, 380, 112, 502, 436, 493, 235, 527, 519,  58,\n",
       "       448, 163, 142, 343, 459, 498,  60,  26], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_idx = ranked_indices[:int(num_label_issues)]\n",
    "issue_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of the highest-ranked data points (most likely to be label issues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>letter_grade</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>338cae</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>7cb11e</td>\n",
       "      <td>78</td>\n",
       "      <td>57</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>d77a5c</td>\n",
       "      <td>89</td>\n",
       "      <td>70</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>4065e7</td>\n",
       "      <td>96</td>\n",
       "      <td>75</td>\n",
       "      <td>92</td>\n",
       "      <td>great final presentation +10</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>85b1fe</td>\n",
       "      <td>72</td>\n",
       "      <td>78</td>\n",
       "      <td>69</td>\n",
       "      <td>missed homework frequently -10</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stud_ID  exam_1  exam_2  exam_3                           notes  \\\n",
       "541  338cae       0      89      90      cheated on exam, gets 0pts   \n",
       "409  7cb11e      78      57      85                             NaN   \n",
       "196  d77a5c      89      70      74                             NaN   \n",
       "215  4065e7      96      75      92    great final presentation +10   \n",
       "206  85b1fe      72      78      69  missed homework frequently -10   \n",
       "\n",
       "    letter_grade noisy_letter_grade  \n",
       "541            D                  A  \n",
       "409            C                  A  \n",
       "196            C                  C  \n",
       "215            A                  A  \n",
       "206            D                  D  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.iloc[ranked_indices[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrvJHkPzSq6Q"
   },
   "source": [
    "# How'd We Do?\n",
    "\n",
    "Let's go a step further and see how we did at automatically identifying which data points are mislabeled. If we take the intersection of the labels errors identified by Confident Learning and the true label errors, we see that our approach was able to identify 83% of the label errors correctly (based on predictions from a model that is only 79% accurate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9O2a6urWc1DA",
    "outputId": "f88b5ce6-33f2-4ef6-e774-19013c33f0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of errors found: 82.9%\n"
     ]
    }
   ],
   "source": [
    "# Computing percentage of true errors identified. \n",
    "true_error_idx = df_train[df_train.letter_grade != df_train.noisy_letter_grade].index.values\n",
    "cl_acc = len(set(true_error_idx).intersection(set(issue_idx)))/len(true_error_idx)\n",
    "print(f\"Percentage of errors found: {round(cl_acc*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzxXoDOqSzn-"
   },
   "source": [
    "# Train a More Robust Model\n",
    "\n",
    "Now that we have the indices of potential label errors within our data, let's remove them from our data, retrain our model, and see what improvement we can gain.\n",
    "\n",
    "Keep in mind that our baseline model from above, trained on the original data using the `noisy_letter_grade` as the prediction label, achieved an accuracy of 79.2%.\n",
    "\n",
    "Let's use a very simple method to handle these label errors and just **drop them entirely** from the data and retrain our exact same `XGBClassifier`. In a real-world application, a better approach might be to have humans review the issues and _correct_ the labels rather than dropping the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsQFmy7xgSUa",
    "outputId": "a33e17ab-c197-4f95-c9c1-0473c16af313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data: 79.2%\n",
      "Accuracy with errors found by Confident Learning removed: 86.4%\n",
      "Reduction in error: 34.7%\n"
     ]
    }
   ],
   "source": [
    "# Remove the label errors found by Confident Learning from the train set\n",
    "data = df_train.drop(issue_idx)\n",
    "filtered_labels = data['noisy_letter_grade']\n",
    "data = data.drop(['stud_ID', 'letter_grade', 'noisy_letter_grade'], axis=1)\n",
    "\n",
    "# Train a more robust classifier with less erroneous data\n",
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "model.fit(data, filtered_labels)\n",
    "# Evaluate on unmodified test set\n",
    "preds = model.predict(test_data)\n",
    "acc_clean = accuracy_score(preds, test_labels)\n",
    "print(f\"Accuracy with original data: {round(acc_original*100, 1)}%\")\n",
    "print(f\"Accuracy with errors found by Confident Learning removed: {round(acc_clean*100, 1)}%\")\n",
    "\n",
    "# Compute reduction in error.\n",
    "err = ((1-acc_original)-(1-acc_clean))/(1-acc_original)\n",
    "print(f\"Reduction in error: {round(err*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J9clVf1UzQZ"
   },
   "source": [
    "After removing the suspected label issues, our model's new accuracy is now 86%, which means we **reduced the error-rate of the model by 35%** (the original model had 79% accuracy). \n",
    "\n",
    "**Note: throughout this entire process we never changed any code related to model architecture/hyperparameters, training, or data preprocessing!  This improvement is strictly coming from increasing the quality of our data which leaves additional room for additional optimizations on the modeling side.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W-Lo82SVp7I"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "For the student grades dataset, we found that simply dropping identified label errors and retraining the model resulted in a 35% reduction in prediction error on our classification problem (with accuracy improving from 79% to 86%).\n",
    "\n",
    "An implementation of the Confident Learning algorithm (and much more) is available in the [cleanlab](https://github.com/cleanlab/cleanlab) library on GitHub. This is how today's lab assignment can be done in a single line of code with Cleanlab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcleanlab\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m cl_issue_idx \u001b[38;5;241m=\u001b[39m cleanlab\u001b[38;5;241m.\u001b[39mfilter\u001b[38;5;241m.\u001b[39mfind_label_issues(\u001b[43mlabels\u001b[49m, pred_probs, return_indices_ranked_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_confidence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "import cleanlab\n",
    "\n",
    "cl_issue_idx = cleanlab.filter.find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cl_issue_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_c\u001b[38;5;241m.\u001b[39miloc[\u001b[43mcl_issue_idx\u001b[49m[:\u001b[38;5;241m5\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cl_issue_idx' is not defined"
     ]
    }
   ],
   "source": [
    "df_c.iloc[cl_issue_idx[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Advanced topic_: you might notice that the above `cl_issue_idx` differs in length (by a little bit) from our `issue_idx`. The reason for this is that we implemented a slightly simplified version of the algorithm in this lab. We skipped a calibration step after computing the confident joint that makes the confident joint have the true noisy prior $p(labels)$ (summed over columns for each row) and also add up to the total number of examples. If you're interested in the details of this, see equation 3 and the subsequent explanation in the [paper](https://jair.org/index.php/jair/article/view/12125)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
